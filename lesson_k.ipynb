{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson_k.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1OmobE1C2dy"
      },
      "source": [
        "#Handwritten digit detection with a DNN\n",
        "\n",
        "In this lesson, we will do more machine learning. We will configure, train, and use a *Deep Neural Network* (DNN) to recognize handwritten digits. This is a step up from the face detection in the last lesson, because this code is able to recognize *which* digit an image of a handwriten character represents. The previous lesson only found faces; it did not recognize who the faces belong to.\n",
        "\n",
        "The code below will use Tensor Flow and Keras, Python modules designed for machine learning applications. We will still use `skimage` to display images, so our first cell is the familiar configuration code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv7Uu7T2CfLM"
      },
      "source": [
        "# one-time imports and configuration\n",
        "import skimage.io\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 150\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6JeQN9pD-17"
      },
      "source": [
        "For this lesson, we will use the *Modified National Institute of Standards and Technology* (MNIST) database. According to this <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">Wikipedia article</a>, the MNIST database is \"... a large database of handwritten digits that is commonly used for training various image processing systems.\" The same article has a sample image showing some of the scanned digits:\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" alt=\"Sample MNIST digits\">\n",
        "\n",
        "Our first step is to access this database. Luckily, the `tensorflow` module has convenience methods to allow us to access the data, as shown in the next cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHq3V7qwBSv7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# reference to the mnist dataset object\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# load the data, save the data in training and testing X and y\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TgsmaExFe7h"
      },
      "source": [
        "The `mnist.load_data()` method call downoads the dataset to our environment, and splits the data into four parts, which we have labeled `X_train`, `y_train`, `X_test`, and `y_test`. Let us explain what those four parts are. \n",
        "\n",
        "In machine-learning-speak, an uppercase `X` represents the things our code is trying to learn how to recognize; in this case, `X` represents 28x28 pixel grayscale images, each of which is an image of a single handwritten digit.\n",
        "\n",
        "A lowercase `y` represents the *labels* associated with each of the items in `X`. In MNIST, `y` contains the actual digit represented by each of the images in `X`. \n",
        "\n",
        "We will deal with `test` versus `train` momentarily. Right now, let us look at a single image from `X` and its associated label from `y`. The next cell selects a single image from `X_train` (the one at index 120 in the `X_train` list) and displays it, using familiar `skimage` tools. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oye45bETCXza"
      },
      "source": [
        "img = X_train[120]\n",
        "skimage.io.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEyaIbtyHmQA"
      },
      "source": [
        "You can see that the image is a (rather sloppy) handwritten '2'. You will also note that the image is inverted, with white \"ink\" on a black background. This is done to make training of the DNN easier; we will not delve into the reasons why here. But, if we want to see the digit image in a more familiar way, it is easy enough to do so!\n",
        "\n",
        "The next cell shows how to invert the pixels in the image, using the `skimage.util` `invert()` function. Displaying the inverted image should result in a more familiar image of a handwritten '2'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I9yT32UIZIS"
      },
      "source": [
        "import skimage.util\n",
        "\n",
        "# invert and display the image so it looks more natural to \n",
        "# human eyes\n",
        "img = skimage.util.invert(img)\n",
        "skimage.io.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYuaJlJjJKpg"
      },
      "source": [
        "The DNN we will construct is supposed to classify these kinds of images -- so, for the image above, it should recognize it as the number two. The `y` lists contain the actual numbers associated with each of the images. In the next cell we verify this by printing the value and type of `y` value associated with the handwrtten two we displayed above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y9tnxlpCu9p"
      },
      "source": [
        "print('Type of y_train[120]:', type(y_train[120]))\n",
        "print('Value of y_train[120]:', y_train[120])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L8bx1TJJqc4"
      },
      "source": [
        "As we can see, the label value for the image above is an integer (an unsigned, 8-bit integer to be precise) with the value 2. \n",
        "\n",
        "Now, let us turn our attention to the significance of the `_test` and `_train` suffixes in the data returned by `mnist.load_data()`.\n",
        "\n",
        "A DNN is \"trained\" by showing it thousands of images and their associated labels, and tweaking the values of hundreds of thousands of numbers in a complicated structure that mimics how neurons in human brains work. Eventually, the DNN \"learns\" the training data, so that it is able to recognize the data in the training set. \n",
        "\n",
        "But, we run the risk of creating a DNN that is too well trained on the training data -- that is, it might be able to recognize the elements of the training set, but not do well at all on unfamiliar data. We want our DNN to work on images it has never seen before. \n",
        "\n",
        "That is where the test set comes in. We reserve some of our images and labels at the outset, and do not use these for training. Instead, use use the (unfamiliar) data in the test set to evaluate the DNN after it has been trained. \n",
        "\n",
        "So, `X_train` and `y_train` are the training sets used to train the DNN, while `X_test` and `y_test` are used to evaluate how good our model is after it has been trained. \n",
        "\n",
        "The next cell looks at the sizes of each of these sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgysLOo2I92p"
      },
      "source": [
        "print('Size of X-train and y_train:', len(X_train), len(y_train))\n",
        "print('Size of X_test and y_test:', len(X_test), len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RqnA8yNMJg3"
      },
      "source": [
        "We can see that we have 60,000 training images and labels, and 10,000 testing images and labels. This is one of the things that makes training DNNs difficult -- the need for large quantities of labeled data. In other words, human beings had to create the labels for these 70,000 sample images, so that the labeled images can be used to train a DNN. \n",
        "\n",
        "The next step is to construct and train the DNN. First, we change the representation of our images from integer numbers in $\\left[0, 255\\right]$ to floating point numbers in $\\left[0, 1\\right]$. This is another numerical trick that helps the DNN work better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfX8a7qiCNGT"
      },
      "source": [
        "# Scale the feature data to be between 0 and 1. \n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s_wLoxVO7Df"
      },
      "source": [
        "## Creating and training the DNN\n",
        "\n",
        "Now, we define the structure of our DNN. A DNN is structured in a series of layers. Inputs (small images of handwritten digits in our case) go into the input later, and on the other end, the classification of the input (a number in $\\left[0, 9\\right]$) appears. The particulars of the layers are well beyond the scope of this lesson, but you can see from the code below that we have four layers in our DNN, and that the input has $78 \\times 78 = 784$ input nodes, one for each pixel in an image we're trying to classify. There are two \"hidden\" layers that form sort of a funnel -- see how the number of nodes goes from 784 to 512 to 256? The final layer has 10 output nodes, one for each of the digits in $\\left[0, 9\\right]$. Conceptually, one of those output nodes will be \"activated\" after we feed an image into the DNN, showing us which digit the network thinks was contained in the image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U7A2e5JM5B4"
      },
      "source": [
        "\n",
        "# Define the DNN model:\n",
        "#    784-node input layer\n",
        "#    512-node hidden layer with ReLU activation\n",
        "#    256-node hidden layer with ReLU activation\n",
        "#    10-node output layer using softmax activation\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK7h7J23QV81"
      },
      "source": [
        "Our next step is to convert the model into something that can be quickly trained, by compiling it. In this step we include parameters that specify how the DNN will be trained and how its performance during training will be evaluated. Again, the specifics of the choices in the next code cell are beyond our scope here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af3OtzyqQUQW"
      },
      "source": [
        "# Compile the model into an executable form\n",
        "# optimizer = method of optimizing the weights\n",
        "# loss = function used to evaluate error\n",
        "# metrics = what measure loss is computed over\n",
        "model.compile(optimizer='sgd', \n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjgWiTFeQxCl"
      },
      "source": [
        "Now, we get to train the model! This will take a while, probably several minutes. As the model trains, you will be able to watch its degree of accuracy -- how frequently it is able to correctly classify training images -- increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfC0Woa1Qvlv"
      },
      "source": [
        "# Train the model; an epoch is a complete training pass\n",
        "# through the entire dataset. \n",
        "model.fit(X_train, y_train, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPDMV9asRVqW"
      },
      "source": [
        "When we ran the training, we consistently created DNNs with over 97% accuracy on the training data. But, how well does the DNN do on the test data? We can use the test images and labels to see, as shown in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBLi94OXBak3"
      },
      "source": [
        "# code to evaluate the trained DNN, using the test data\n",
        "print(model.metrics_names, model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCo5GBIQRve2"
      },
      "source": [
        "Our model achieved almost 97% accuracy on the test data! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CEvsc4ASOb0"
      },
      "source": [
        "## Using the trained DNN on a new image\n",
        "\n",
        "Now, let's try to put the trained DNN to use. In the next cell, we access an image of a hand-drawn digit -- a number drawn on paper and then photographed -- and display it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnpO7kg8B11V"
      },
      "source": [
        "# load and display original image\n",
        "img = skimage.io.imread('https://i.imgur.com/cV7uAMx.jpg')\n",
        "skimage.io.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh9YNGo4Sug2"
      },
      "source": [
        "We have to turn this into an image suitable for our DNN - we need to crop to select only the '8' from the image, convert it to grayscale, invert the image, and resize so the image is 28x28 pixels in size. The next cell does this, based on some coordinates we derived from an image editing program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_qi25EKSrm-"
      },
      "source": [
        "# crop to include just the 8\n",
        "img = img[1117:1761, 501:1157, :]\n",
        "\n",
        "# convert to grayscale\n",
        "import skimage.color\n",
        "img = skimage.color.rgb2gray(img)\n",
        "\n",
        "# resize image to 28 x 28 pixels\n",
        "from skimage.transform import resize\n",
        "img = resize(img, (28, 28), anti_aliasing = True)\n",
        "\n",
        "# invert the image\n",
        "img = skimage.util.invert(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUonBuRWVJYA"
      },
      "source": [
        "Let's see how our image looks now!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzU3IFFTlGk"
      },
      "source": [
        "# display our new image\n",
        "skimage.io.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG86kfRIXBXL"
      },
      "source": [
        "Now we can use the DNN, and see if it properly categorizes the image! We do that by calling the `model.predict()` method call. The parameter we pass in is our image, which we have to reshape just a bit in order to match the data we trained the DNN on. \n",
        "\n",
        "In particular, if we execute the command `X_train.shape`, we see the output `(60000, 28, 28)`, which means the training images were actually stored in a 3D array, where each \"sheet\" of the array is a complete image. We need to make our image into a similar shape, by calling `img.reshape(1, 28, 28)`. That makes the image into a 3D array, where the first -- and only -- sheet has our image. \n",
        "\n",
        "The `model.predict()` call will return an array of predictions, in this case, showing the probability that the image is associated with a particular digit. In the case of this exercise, the indices of the probabilities correspond to the digits. The next cell shows how to fetch the predicted digit out of that array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5d0x9mATqHL"
      },
      "source": [
        "predictions = model.predict(img.reshape(1, 28, 28))\n",
        "print('Predictions array:', predictions)\n",
        "print('DNN thinks your image is:', predictions.argmax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESmGjjDoW25_"
      },
      "source": [
        "---\n",
        "> **Your turn: recognize one of your handwritten digits**\n",
        "> \n",
        "> Now it's your turn! First, write a single digit on a piece of paper,\n",
        "> photograph it with a digital camera or your smartphone, and then \n",
        "> upload it to the files area of your Google Colab instance. \n",
        "> \n",
        "> Once you have the image uploaded, write code to read the image in,\n",
        "> crop it to have only the digit in the image, conver it to grayscale,\n",
        "> resize it, and invert it, using the techniques shown above.\n",
        ">\n",
        "> Then, ask the `model` object to classify your handwritten digit and \n",
        "> see if it makes the correct decision. \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YXoJE1kfsFO"
      },
      "source": [
        "# TODO: read in, crop, grayscale, resize, and invert your own digit image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEzwNjl2Xpm5"
      },
      "source": [
        "# TODO: use the model to classify your digit\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}