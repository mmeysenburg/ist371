{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson_h.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz_6paLLzxdI"
      },
      "source": [
        "#Connected Component Analysis\n",
        "\n",
        "In the thresholding episode we covered dividing an image in foreground and background pixels. In the junk example image, we considered the colored shapes as foreground objects on a white background.\n",
        "\n",
        "<img src=\"https://datacarpentry.org/image-processing/fig/06-junk-before.jpg\" alt=\"paper shapes\" style=\"float: left; margin-right:10px;\"/>\n",
        "\n",
        "In thresholding we went from the original image to this version:\n",
        "\n",
        "<img src=\"https://datacarpentry.org/image-processing/fig/06-junk-mask.png\" alt=\"binary paper shapes\" style=\"float: left; margin-right:10px;\"/>\n",
        "\n",
        "Here, we created a mask that only highlights the parts of the image that we find interesting, namely, the objects. All objects have pixel value of `True` while the background pixels are `False`.\n",
        "\n",
        "By looking at the mask image, one can count the objects that are present in the image (7). But how could Python code do that? I.e., how can a program decide which lump of pixels constitutes a single object?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TEUJidR0anv"
      },
      "source": [
        "##Pixel Neighborhoods\n",
        "\n",
        "In order to decide which pixels belong to the same object, one can exploit their neighborhood: pixels that are directly next to each other and belong to the foreground class can be considered to belong to the same object.\n",
        "\n",
        "Let's consider the following mask \"image\" with 8 rows, and 8 columns. Note that for brevity, 0 is used to represent False (background) and 1 to represent True (foreground).\n",
        "\n",
        "```\n",
        "0 0 0 0 0 0 0 0\n",
        "0 1 1 0 0 0 0 0\n",
        "0 1 1 0 0 0 0 0\n",
        "0 0 0 1 1 1 0 0\n",
        "0 0 0 1 1 1 1 0\n",
        "0 0 0 0 0 0 0 0\n",
        "```\n",
        "\n",
        "As expected, the pixels are organized in a rectangular grid. In order to understand pixel neighborhoods we will introduce the concept of \"jumps\" between pixels. The jumps follow two rules: First, one jump is only allowed along the column, or the row. Diagonal jumps are not allowed. Consider the small image diagram below. From the center pixel, denoted with `o`, only the pixels indicated with an `x` are reachable:\n",
        "\n",
        "```\n",
        "- x -\n",
        "x o x\n",
        "- x -\n",
        "```\n",
        "\n",
        "The pixels on the diagonal (from `o`) are not reachable with a single jump, and this denoted by `-` in the sample. The pixels reachable with a single jump form the *1-jump neighborhood*.\n",
        "\n",
        "The second rule states that in a sequence of jumps, one may only jump in row and column direction once. These are called *orthogonal* jumps. An example of a sequence of orthogonal jumps is shown below. Starting from `o`, the first jump, to the pixel labeled `1`, goes along the row to the right. The second jump, to the pixel labeled `2`, then goes along the column direction up. After this the sequence cannot be continued as a jump has been made in row and column direction.\n",
        "\n",
        "```\n",
        "- - 2\n",
        "- o 1\n",
        "- - -\n",
        "```\n",
        "\n",
        "All pixels reachable with one, or two jumps form the *2-jump neighborhood*. The grid below illustrates the pixels reachable from the center pixel `o` with a single jump, highlighted with a `1`, and the pixels reachable with 2 jumps with a `2`.\n",
        "\n",
        "```\n",
        "2 1 2\n",
        "1 o 1\n",
        "2 1 2\n",
        "```\n",
        "\n",
        "For the 8x8 binary image introduced above, we can apply the two different neighborhood rules. With single jump connectivity for each pixel, we get two resulting objects, highlighted in the image with `A`'s and `B`'s.\n",
        "\n",
        "```\n",
        "0 0 0 0 0 0 0 0\n",
        "0 A A 0 0 0 0 0\n",
        "0 A A 0 0 0 0 0\n",
        "0 0 0 B B B 0 0\n",
        "0 0 0 B B B B 0\n",
        "0 0 0 0 0 0 0 0\n",
        "```\n",
        "\n",
        "In the 1-jump version, only pixels that are neighbors in rows or columns are considered connected. However, if we use 2-jump connectivity, two objects may be considered connected diagonally, if they are close enough. Two jump connectivity for the same 8x8 binary image is illustrated below.\n",
        "\n",
        "```\n",
        "0 0 0 0 0 0 0 0\n",
        "0 A A 0 0 0 0 0\n",
        "0 A A 0 0 0 0 0\n",
        "0 0 0 A A A 0 0\n",
        "0 0 0 A A A A 0\n",
        "0 0 0 0 0 0 0 0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyjQijYz3VoG"
      },
      "source": [
        "---\n",
        "> **Exercise: Practice object counting**\n",
        ">\n",
        "> Consider this 8x8 binary image:\n",
        ">\n",
        "> ```\n",
        "> 0 0 0 0 0 0 0 0\n",
        "> 0 1 0 0 0 1 1 0\n",
        "> 0 0 1 0 0 0 0 0\n",
        "> 0 1 0 1 1 1 0 0\n",
        "> 0 1 0 1 1 0 0 0\n",
        "> 0 0 0 0 0 0 0 0\n",
        "> ```\n",
        "> \n",
        "> How many objects are in the image if we use a 1-jump neighborhood for \n",
        "> connectivity? \n",
        "> \n",
        "> How many objects are in the image if we use a 2-jump neighborhood for \n",
        "> connectivity? \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVHh1H6W4Pau"
      },
      "source": [
        "The 1-jump and 2-jump neighborhoods may be referred to as 4- and 8-neighborhoods, respectively. This is because, with a 1-jump neighborhood, you can reach four pixels from a given starting point, while with a 2-jump neighborhood, you can reach eight pixels. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97bcUZlH4nfd"
      },
      "source": [
        "##Connected Component Analysis\n",
        "\n",
        "In order to find the objects in an image, we want to employ an operation that is called *Connected Component Analysis (CCA)*. This operation takes a binary image (like the ones produced by thresholding) as an input. Usually, the `False` value in this image is associated with background pixels, and the `True` value indicates foreground, or object pixels. Given a thresholded image, CCA produces a new labeled image with integer pixel values. Pixels with the same value belong to the same object.\n",
        "\n",
        "Let's begin to use Python code and CCA to count the number of objects in an image. We start with some familiar one-time imports, and a new one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "job9dVPgzuEW"
      },
      "source": [
        "# one-time imports and configuration\n",
        "import numpy as np\n",
        "import skimage.color\n",
        "import skimage.filters\n",
        "import skimage.io\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 150\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# import for CCA\n",
        "import skimage.measure\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OitQCFO6509"
      },
      "source": [
        "Then we define values for thresholding, load the original image as grayscale, blur it, threshold it, and display it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlEUeBnp63oG"
      },
      "source": [
        "sigma = 2.0\n",
        "threshold = 0.9\n",
        "\n",
        "image = skimage.io.imread('https://i.imgur.com/c1Y4NyB.jpg', as_gray=True)\n",
        "blur = skimage.filters.gaussian(image, sigma)\n",
        "binary = blur < threshold\n",
        "\n",
        "skimage.io.imshow(binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BEZ5rmi8V0N"
      },
      "source": [
        "Now we can perform CCA on the binary image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wXcRQWV7i4m"
      },
      "source": [
        "# Perform CCA on the binary image\n",
        "labeledImage, num = skimage.measure.label(binary, connectivity=2, return_num=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsepwokE8knk"
      },
      "source": [
        "The `skimage.measure.label` function performs the CCA process. This function takes the image to perform CCA on (`binary` in this case), the `connectivity` we wish to use (2-jump neighborhood connectivity here), and a flag stating that we want to know the number of objects found (`return_num=True`). The function returns a tuple containing a new image, where each object is represented by a unique pixel value, and the number of objects found. Here we are saving that new image in a variable named `labeledImage`, and the number of objects found in `num`.\n",
        "\n",
        "Let's display `labeledImage` and see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq8UCTdM8jfZ"
      },
      "source": [
        "skimage.io.imshow(labeledImage)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22k09f0Z-3WM"
      },
      "source": [
        "In the displayed image, all of the large objects seem to be correctly identified, and are shown with different colors. If you look closely, there do seem to be some other \"objects\" in the display that were not really objects in the original image. We shall return to that shortly.\n",
        "\n",
        "It is possible that the image displayed by the preceding code does not appear to show the objects. This could happen, depending on the system executing the code, because of the underlying data type of the labeled image and the small number of objects in the image. If that happens, we can remap the labeled image back to our RGB color space and re-display it, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bs5cJ6I9Y-N"
      },
      "source": [
        "# only necessary if the preceding display did not work well\n",
        "coloredLabelImage = skimage.color.label2rgb(labeledImage, bg_label=0)\n",
        "skimage.io.imshow(coloredLabelImage)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoF6fBq8By1N"
      },
      "source": [
        "Now, let's return to the fact that there seem to be some very tiny objects found by the CCA process. Let's see how many objects were detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0aLZXNk-yDR"
      },
      "source": [
        "print('Number of objects:', num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDq98CMJCLij"
      },
      "source": [
        "Our code is overcounting! It has found 4 \"objects\" that did not really exist in the image; these are likely due to either abnormalities in the background of the image, or due to noise that was not smoothed over by the blurring step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsmQiB9JCftb"
      },
      "source": [
        "---\n",
        "> **Vary parameters to improve accuracy**\n",
        "> \n",
        "> The object-counting code from the previous cells has been consolidated\n",
        "> into the cell below. Experiment with the blurring (`sigma`) and \n",
        "> thresholding (`threshold`) parameters to try to correctly count the\n",
        "> number of objects in the image.\n",
        "> \n",
        "> Report the parameters you found that worked best, and speculate on\n",
        "> the impact increasing / decreasing each parameter has on the number of\n",
        "> objects found.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vozQolKCCFQ"
      },
      "source": [
        "# TODO: vary these two parameters to produce the best object count\n",
        "sigma = 2.0\n",
        "threshold = 0.9\n",
        "\n",
        "image = skimage.io.imread('https://i.imgur.com/c1Y4NyB.jpg', as_gray=True)\n",
        "blur = skimage.filters.gaussian(image, sigma)\n",
        "binary = blur < threshold\n",
        "\n",
        "# Perform CCA on the binary image\n",
        "labeledImage, num = skimage.measure.label(binary, connectivity=2, return_num=True)\n",
        "\n",
        "# Display labeled image and number of objects\n",
        "skimage.io.imshow(labeledImage)\n",
        "print('Number of objects:', num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS7AMhU0Du0z"
      },
      "source": [
        "##Morphometrics: Describing Object Features with Numbers\n",
        "\n",
        "We certainly do not want to have to try to fine-tune the blurring and threshold parameters for each image we process, in order to determine the right number of objects contained by the image. A better approach is to *programatically* filter out the \"objects\" detected by CCA that are not, in fact, objects at all. One way we could do this is to examine the areas of the detected objects, and discard the tiny objects as probable artefacts of noise in the image. Here, we are relying on the fact that the real objects in our image are much, much bigger than the incorrectly-identified objects. \n",
        "\n",
        "First, let's go back to the original `sigma` and `threshold` values from the start of the lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKBjdczrG6yT"
      },
      "source": [
        "sigma = 2.0\n",
        "threshold = 0.9\n",
        "\n",
        "image = skimage.io.imread('https://i.imgur.com/c1Y4NyB.jpg', as_gray=True)\n",
        "blur = skimage.filters.gaussian(image, sigma)\n",
        "binary = blur < threshold\n",
        "\n",
        "# Perform CCA on the binary image\n",
        "labeledImage, num = skimage.measure.label(binary, connectivity=2, return_num=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc5k25xTG9Vu"
      },
      "source": [
        "The next cell shows how we could begin the process of filtering out false objects, by drawing a histogram of number of times areas are found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l42SVOFiDqxL"
      },
      "source": [
        "# get the properties of the labeled image\n",
        "objectFeatures = skimage.measure.regionprops(labeledImage)\n",
        "\n",
        "# create a list of areas\n",
        "objectAreas = [objF['area'] for objF in objectFeatures]\n",
        "\n",
        "# show a histogram for the list of areas\n",
        "plt.hist(objectAreas)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpI3G6RiFvta"
      },
      "source": [
        "This histogram shows us that there are four objects with areas less than 100000 -- those must be the four falsely identified objects!\n",
        "\n",
        "To get there, we first call `skimage.measure.regionprops()` on `labeledImage`. This returns, in this case, a list of 11 `RegionProperties` objects, one for each object in the image that has been found. The `RegionProperties` object contains volumes of information about the corresponding object in the image. \n",
        "\n",
        "The one we are interested in here is the `area` property, which is the number of pixels in the connected image object. We access each `RegionProperties` value as we do with Python dictionaries.\n",
        "\n",
        "So, our next step is to produce a list of the areas of the objects in the image. This is done via list comprehension,\n",
        "\n",
        "```\n",
        "objectAreas = [objF['area'] for objF in objectFeatures]\n",
        "```\n",
        "\n",
        "For each `RegionProperties` object returned by the `skimage.measure.regionpropos()` function, we access the `area` property, and append it to the end of a list named `objectAreas`. \n",
        "\n",
        "Then the code displays the histogram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXA5Y4qxI0D7"
      },
      "source": [
        "---\n",
        "> **Ignoring small objects programatically**\n",
        "> \n",
        "> Modify the code in the cell below to only count the large objects\n",
        "> in the image. *Hint:* find the mean area of the detected objects, and \n",
        "> only count objects larger than 25% of the mean.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Yyx-oAFrOz"
      },
      "source": [
        "sigma = 2.0\n",
        "threshold = 0.9\n",
        "\n",
        "image = skimage.io.imread('https://i.imgur.com/c1Y4NyB.jpg', as_gray=True)\n",
        "blur = skimage.filters.gaussian(image, sigma)\n",
        "binary = blur < threshold\n",
        "\n",
        "# Perform CCA on the binary image\n",
        "labeledImage, num = skimage.measure.label(binary, connectivity=2, return_num=True)\n",
        "\n",
        "# get the properties of the labeled image\n",
        "objectFeatures = skimage.measure.regionprops(labeledImage)\n",
        "\n",
        "# create a list of areas\n",
        "objectAreas = [objF['area'] for objF in objectFeatures]\n",
        "\n",
        "# TODO: print number of large objects\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFvQpUWbKUcC"
      },
      "source": [
        "---\n",
        "> **Visualize only large objects**\n",
        ">\n",
        "> Now, make modifications to `labeledImage` so that only the large \n",
        "> objects are displayed. *Hint:* recall that each object in \n",
        "> `labeledImage` contains pixels that are all the same value. Also, this\n",
        "> label value can be accessed from the `RegionProperties` object via the\n",
        "> `'label'` key.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apwHQWo5KNk7"
      },
      "source": [
        "sigma = 2.0\n",
        "threshold = 0.9\n",
        "\n",
        "image = skimage.io.imread('https://i.imgur.com/c1Y4NyB.jpg', as_gray=True)\n",
        "blur = skimage.filters.gaussian(image, sigma)\n",
        "binary = blur < threshold\n",
        "\n",
        "# Perform CCA on the binary image\n",
        "labeledImage, num = skimage.measure.label(binary, connectivity=2, return_num=True)\n",
        "\n",
        "# get the properties of the labeled image\n",
        "objectFeatures = skimage.measure.regionprops(labeledImage)\n",
        "\n",
        "# create a list of areas\n",
        "objectAreas = [objF['area'] for objF in objectFeatures]\n",
        "\n",
        "# TODO: modify labeledImage so only the large objects are shown\n",
        "\n",
        "\n",
        "\n",
        "skimage.io.imshow(labeledImage)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dSf6XcFMEvR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}